#!/usr/bin/env python3
"""
RLVCR Confidenceè®¡ç®—éªŒè¯è¡¥ä¸
å¯ä»¥ç›´æ¥æ’å…¥åˆ° _compute_confidence_chunk å‡½æ•°ä¸­
"""

verification_code = '''
# åœ¨ _compute_confidence_chunk å‡½æ•°çš„è°ƒè¯•éƒ¨åˆ†æ·»åŠ è¿™æ®µä»£ç ï¼š
# ä½ç½®ï¼šåœ¨è®¡ç®—å®Œprefix_probsä¹‹åï¼Œæ‰“å°confidenceä¹‹å‰

if debug_mode and prefix_idx == 0:  # åªéªŒè¯ç¬¬ä¸€ä¸ªprefixï¼Œé¿å…è¿‡å¤šè¾“å‡º
    print(f"\\nğŸ§ª TOKENIZATION & EQUIVALENCE VERIFICATION:")
    
    # 1. éªŒè¯tokenizationä¸€è‡´æ€§
    current_action_tokens = action_token_ids
    action_text = self.tokenizer.decode(action_token_ids, skip_special_tokens=True)
    
    print(f"  Action text: '{action_text}'")
    print(f"  Current method tokens: {current_action_tokens}")
    print(f"  Token texts: {[self.tokenizer.decode([t]) for t in current_action_tokens]}")
    
    # ç›´æ¥æ–¹æ³•tokenization
    try:
        full_input_text = input_prefixes[prefix_idx] + action_text
        full_tokens = self.tokenizer.encode(full_input_text, add_special_tokens=False)
        prefix_tokens = self.tokenizer.encode(input_prefixes[prefix_idx], add_special_tokens=False)
        
        prefix_len = len(prefix_tokens)
        direct_action_tokens = full_tokens[prefix_len:]
        
        print(f"  Direct method tokens: {direct_action_tokens}")
        print(f"  Direct token texts: {[self.tokenizer.decode([t]) for t in direct_action_tokens]}")
        
        tokens_match = (current_action_tokens == direct_action_tokens)
        print(f"  Tokenization match: {tokens_match}")
        
        if not tokens_match:
            print(f"  âŒ TOKENIZATION MISMATCH!")
            print(f"     This is likely the source of 'strange results'")
            print(f"     Current approach calculates probabilities for different tokens!")
            
            # åˆ†æå·®å¼‚
            print(f"\\n  ğŸ“Š Detailed token comparison:")
            max_len = max(len(current_action_tokens), len(direct_action_tokens))
            for i in range(max_len):
                curr_token = current_action_tokens[i] if i < len(current_action_tokens) else None
                direct_token = direct_action_tokens[i] if i < len(direct_action_tokens) else None
                curr_text = self.tokenizer.decode([curr_token]) if curr_token else None
                direct_text = self.tokenizer.decode([direct_token]) if direct_token else None
                
                if curr_token != direct_token:
                    print(f"     Position {i}: {curr_token}('{curr_text}') vs {direct_token}('{direct_text}') âŒ")
                else:
                    print(f"     Position {i}: {curr_token}('{curr_text}') âœ…")
        else:
            print(f"  âœ… Tokenization is consistent")
            
    except Exception as e:
        print(f"  âŒ Verification failed: {e}")
    
    # 2. æ¦‚ç‡è®¡ç®—ç­‰ä»·æ€§éªŒè¯ï¼ˆå¦‚æœtokenizationä¸€è‡´ï¼‰
    if 'tokens_match' in locals() and tokens_match:
        print(f"\\n  ğŸ” Probability calculation verification:")
        print(f"     Current method probabilities: {[f'{p:.6f}' for p in prefix_probs]}")
        print(f"     Min probability: {min(prefix_probs):.6f}")
        print(f"     Geometric mean: {(np.prod(prefix_probs)**(1/len(prefix_probs))):.6f}")
        print(f"     Arithmetic mean: {np.mean(prefix_probs):.6f}")
        
        # å»ºè®®ï¼šå¦‚æœæƒ³è¦æ›´ç¨³å®šçš„confidence measure
        print(f"\\n  ğŸ’¡ Alternative confidence measures:")
        joint_prob = np.prod(prefix_probs)
        geom_mean = joint_prob**(1/len(prefix_probs))
        print(f"     Joint probability: {joint_prob:.8f}")
        print(f"     Geometric mean: {geom_mean:.6f}")
        print(f"     Current (min): {min(prefix_probs):.6f}")
'''

print("ğŸ”§ RLVCRéªŒè¯è¡¥ä¸")
print("=" * 50)
print()
print("å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°ä½ çš„ _compute_confidence_chunk å‡½æ•°ä¸­ï¼š")
print("ä½ç½®ï¼šåœ¨è®¡ç®—å®Œ prefix_probs ä¹‹åï¼Œæ‰“å° confidence ä¹‹å‰")
print()
print(verification_code)
print()
print("ğŸ¯ è¿™ä¸ªè¡¥ä¸ä¼šéªŒè¯ï¼š")
print("1. Tokenizationä¸€è‡´æ€§ - æ£€æŸ¥é€tokenæ–¹æ³•å’Œç›´æ¥æ–¹æ³•çš„tokenåºåˆ—æ˜¯å¦ç›¸åŒ")
print("2. å¦‚æœä¸ä¸€è‡´ï¼Œè¯¦ç»†æ˜¾ç¤ºå·®å¼‚ä½ç½®")
print("3. æä¾›ä¸åŒçš„confidenceè®¡ç®—æ–¹å¼å¯¹æ¯”")
print()
print("ğŸ’¡ å¦‚æœå‘ç°tokenizationä¸åŒ¹é…ï¼Œè¿™å°±æ˜¯'å¥‡æ€ªç»“æœ'çš„æ ¹æºï¼")
print("   è§£å†³æ–¹æ¡ˆï¼šåˆ‡æ¢åˆ°ç›´æ¥è‡ªå›å½’æ–¹æ³•ï¼Œæˆ–è€…ä¿®å¤tokenizationé€»è¾‘")

# é¢å¤–çš„åˆ†æ
print("\n" + "=" * 50)
print("ğŸ” å¯èƒ½çš„Tokenizationé—®é¢˜åŸå› ")
print("=" * 50)

print("""
å¸¸è§çš„tokenizationä¸ä¸€è‡´æƒ…å†µï¼š

1. ğŸ“ ä¸Šä¸‹æ–‡å½±å“åˆ†è¯ï¼š
   - å•ç‹¬: "open door" -> ['open', ' door']  
   - ä¸Šä¸‹æ–‡: "...action>open door" -> ['open', ' door']  # å¯èƒ½ä¸åŒï¼

2. ğŸ”¤ å­è¯åˆ†å‰²å·®å¼‚ï¼š
   - 'outside' å¯èƒ½è¢«åˆ†å‰²ä¸º [' out', 'side'] æˆ– [' outside']
   - å–å†³äºå‰é¢çš„contextå’Œtokenizerçš„çŠ¶æ€

3. ğŸ¯ ç‰¹æ®Štokenå¤„ç†ï¼š
   - '<action>' tagçš„å­˜åœ¨å¯èƒ½å½±å“åç»­tokençš„åˆ†å‰²
   - æŸäº›tokenizerå¯¹ç‰¹æ®Šç¬¦å·æ•æ„Ÿ

4. ğŸ”¢ è¾¹ç•Œæ•ˆåº”ï¼š
   - åºåˆ—é•¿åº¦é™åˆ¶å¯èƒ½å¯¼è‡´æˆªæ–­ä½ç½®ä¸åŒ
   - å½±å“æœ€åå‡ ä¸ªtokençš„åˆ†å‰²

è§£å†³æ–¹æ¡ˆï¼š
âœ… ä½¿ç”¨ç›´æ¥è‡ªå›å½’æ–¹æ³•ï¼ˆæ¨èï¼‰
âœ… æˆ–è€…ç¡®ä¿tokenizationçš„ä¸€è‡´æ€§éªŒè¯
âœ… åœ¨debugæ¨¡å¼ä¸‹æ·»åŠ è¯¦ç»†çš„tokenå¯¹æ¯”
""")

if __name__ == "__main__":
    pass
